{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\earth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load categories news data from nltk\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown \n",
    "corpus = brown.sents(categories=['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the corpus is alreary tokenize \n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liens',\n",
       " \"Hollywood's\",\n",
       " 'editing',\n",
       " 'relief',\n",
       " 'less',\n",
       " 'endeared',\n",
       " '100,000',\n",
       " 'Bernardine',\n",
       " 'boys',\n",
       " 'leaving',\n",
       " 'followed',\n",
       " 'Caucusing',\n",
       " 'baggage',\n",
       " \"Authority's\",\n",
       " 'Corporation',\n",
       " 'Springfield',\n",
       " 'obtain',\n",
       " 'fittest',\n",
       " 'intangibles',\n",
       " 'encounters',\n",
       " 'Gift',\n",
       " 'intercepted',\n",
       " 'father',\n",
       " 'peddler',\n",
       " 'Write',\n",
       " 'understandable',\n",
       " 'Journal-Bulletin',\n",
       " 'Meet',\n",
       " 'study',\n",
       " 'non-farm',\n",
       " 'Rickards',\n",
       " 'agreeing',\n",
       " 'potato',\n",
       " 'healed',\n",
       " 'pushing',\n",
       " 'bloodstream',\n",
       " 'receptive',\n",
       " '1,257,700',\n",
       " 'monks',\n",
       " 'formerly',\n",
       " 'Casals',\n",
       " 'recommendation',\n",
       " 'Mom',\n",
       " 'listening',\n",
       " 'polled',\n",
       " 'count',\n",
       " 'stuffed',\n",
       " 'domination',\n",
       " \"football's\",\n",
       " 'bonds',\n",
       " 'burglarproof',\n",
       " 'correspondents',\n",
       " 'campaigning',\n",
       " 'lazy',\n",
       " 'neatly',\n",
       " 'solo',\n",
       " 'racket',\n",
       " 'McAlester',\n",
       " 'Sheldon',\n",
       " 'occupying',\n",
       " 'danger',\n",
       " 'speck',\n",
       " 'Moller',\n",
       " 'whiz',\n",
       " '3-10',\n",
       " 'peddle',\n",
       " 'bans',\n",
       " 'welcomed',\n",
       " 'central',\n",
       " 'Centredale',\n",
       " \"Bucs'\",\n",
       " 'Cable',\n",
       " \"shop's\",\n",
       " 'crossroads',\n",
       " 'switching',\n",
       " 'bleacher-type',\n",
       " 'Giacometti',\n",
       " '1910',\n",
       " 'ghastly',\n",
       " 'Foliage',\n",
       " 'appraisal',\n",
       " 'give',\n",
       " 'Comedian',\n",
       " \"people's\",\n",
       " 'B.',\n",
       " \"university's\",\n",
       " 'defensive',\n",
       " 'silenced',\n",
       " 'tones',\n",
       " 'shortage',\n",
       " 'framed',\n",
       " 'sandwich',\n",
       " 'Councilwoman',\n",
       " 'mound',\n",
       " 'pieces',\n",
       " 'society',\n",
       " 'intervention',\n",
       " \"janitors'\",\n",
       " 'display',\n",
       " 'antiquarians',\n",
       " 'hiring',\n",
       " 'centralized',\n",
       " 'startling',\n",
       " 'Profili',\n",
       " 'Smithfield',\n",
       " 'Kestner',\n",
       " 'Bishop',\n",
       " 'ten-hour',\n",
       " 'masterful',\n",
       " 'students',\n",
       " \"He's\",\n",
       " 'caskets',\n",
       " 'precipitated',\n",
       " 'Carey',\n",
       " \"darlin'\",\n",
       " 'slice',\n",
       " 'Putt',\n",
       " 'subpenas',\n",
       " 'Lien',\n",
       " 'Unconscionable',\n",
       " 'employments',\n",
       " 'writers',\n",
       " \"Furhmann's\",\n",
       " 'yeast',\n",
       " 'sledding',\n",
       " 'Freedom',\n",
       " 'summitry',\n",
       " 'Blanchard',\n",
       " '10-team',\n",
       " 'ears',\n",
       " 'Benched',\n",
       " 'Pezza',\n",
       " 'Michael',\n",
       " 'Investment',\n",
       " 'spirits',\n",
       " 'Woonasquatucket',\n",
       " 'Crystal',\n",
       " 'prevention',\n",
       " 'Brain',\n",
       " 'Representatives',\n",
       " 'Mills',\n",
       " \"Case's\",\n",
       " 'Siebern',\n",
       " '!',\n",
       " 'knowledge',\n",
       " 'Walbridge',\n",
       " 'combines',\n",
       " 'Composite',\n",
       " 'appealing',\n",
       " 'broaden',\n",
       " 'groove',\n",
       " 'auditorium',\n",
       " 'flicker',\n",
       " 'destroy',\n",
       " 'Collins',\n",
       " \"Ruth's\",\n",
       " 'honoring',\n",
       " 'Mount',\n",
       " 'Shoettle',\n",
       " \"drivers'\",\n",
       " 'shall',\n",
       " 'all-automatic',\n",
       " 'Garson',\n",
       " 'Dignity',\n",
       " 'coal',\n",
       " 'reestablish',\n",
       " 'contributors',\n",
       " 'Coast',\n",
       " 'shoulder',\n",
       " 'organizing',\n",
       " 'coffee',\n",
       " 'Missionary',\n",
       " 'chalk',\n",
       " 'middle-aged',\n",
       " 'endorse',\n",
       " 'entire',\n",
       " 'Ebbetts',\n",
       " 'runs',\n",
       " '$214',\n",
       " '1600',\n",
       " '$700',\n",
       " 'sooner',\n",
       " 'he',\n",
       " 'bolted',\n",
       " 'receiving',\n",
       " 'Anaconda',\n",
       " 'lemonade',\n",
       " 'Wales',\n",
       " 'Streets',\n",
       " 'McGehee',\n",
       " 'mechanics',\n",
       " 'believe',\n",
       " 'Romans',\n",
       " 'McElyee',\n",
       " 'index',\n",
       " 'recipient',\n",
       " 'arms',\n",
       " 'Clayton',\n",
       " 'woo',\n",
       " 'sidemen',\n",
       " 'driveway',\n",
       " 'Increasing',\n",
       " 'exceed',\n",
       " 'arched',\n",
       " 'Romantic',\n",
       " 'carriers',\n",
       " 'her',\n",
       " 'thrill',\n",
       " 'Geraldine',\n",
       " 'incurred',\n",
       " 'Fiedler',\n",
       " 'aged-care',\n",
       " 'crop',\n",
       " 'Olney',\n",
       " 'Pfc.',\n",
       " 'prevent',\n",
       " '12:01',\n",
       " 'Dimaggio',\n",
       " 'unaccustomed',\n",
       " 'currency',\n",
       " 'battling',\n",
       " 'Enforce',\n",
       " 'hospitality',\n",
       " 'upturn',\n",
       " \"grandmother's\",\n",
       " 'settlement',\n",
       " 'Success',\n",
       " '13th',\n",
       " 'scramble',\n",
       " 'May',\n",
       " 'intrigue',\n",
       " 'N.C.',\n",
       " '80',\n",
       " 'sugar',\n",
       " 'Dunkel',\n",
       " 'tapestry',\n",
       " 'traditionally',\n",
       " 'Appointment',\n",
       " 'humor',\n",
       " 'battered',\n",
       " 'Felske',\n",
       " 'electric',\n",
       " 'delight',\n",
       " 'pushed',\n",
       " 'fringed',\n",
       " 'explains',\n",
       " 'ruddy',\n",
       " 'Oregon',\n",
       " 'interpretation',\n",
       " 'chase',\n",
       " 'grassroots',\n",
       " 'Geraghty',\n",
       " 'Whitney',\n",
       " 'pure',\n",
       " \"Howsam's\",\n",
       " 'convenient',\n",
       " 'alternate',\n",
       " 'Venturi',\n",
       " 'Stansbery',\n",
       " 'docile',\n",
       " \"father's\",\n",
       " 'Rickenbaugh',\n",
       " 'Republican-controlled',\n",
       " 'audience',\n",
       " 'manufacturing',\n",
       " 'embarrassing',\n",
       " 'ground',\n",
       " '160,000',\n",
       " 'cited',\n",
       " 'New',\n",
       " 'purchased',\n",
       " 'salesman',\n",
       " 'bids',\n",
       " 'deferred',\n",
       " 'Lang',\n",
       " 'optimism',\n",
       " 'Fisher',\n",
       " 'argument',\n",
       " 'seldom',\n",
       " 'curious',\n",
       " 'Jay',\n",
       " 'until',\n",
       " 'secretarial',\n",
       " 'rose',\n",
       " 'Workshops',\n",
       " 'Universal-International',\n",
       " 'representations',\n",
       " 'Kelsey',\n",
       " 'La.',\n",
       " 'maximum',\n",
       " 'seize',\n",
       " 'combination',\n",
       " 'Fiat',\n",
       " 'acknowledged',\n",
       " 'Their',\n",
       " 'dependency',\n",
       " 'Rozelle',\n",
       " 'Tech.',\n",
       " 'enjoy',\n",
       " 'Patrol',\n",
       " 'piety',\n",
       " 'firms',\n",
       " 'home',\n",
       " 'Merner',\n",
       " 'Taft-Hartley',\n",
       " 'Herzog',\n",
       " 'Chance',\n",
       " 'Malmud',\n",
       " 'establishing',\n",
       " 'Woodland',\n",
       " 'network',\n",
       " 'spelled',\n",
       " 'Sent',\n",
       " 'trailing',\n",
       " 'client',\n",
       " 'Kochanek',\n",
       " 'speaker',\n",
       " 'Salvatore',\n",
       " 'Kercheval',\n",
       " '$451,500',\n",
       " 'Felix',\n",
       " \"salesman's\",\n",
       " 'addresses',\n",
       " 'N.D.',\n",
       " 'dying',\n",
       " 'calendar',\n",
       " 'growth',\n",
       " 'felt',\n",
       " '10th',\n",
       " 'generated',\n",
       " 'matches',\n",
       " 'underground',\n",
       " 'Cubs',\n",
       " 'SMU',\n",
       " 'displaying',\n",
       " 'Benjamin',\n",
       " 'cheap',\n",
       " 'already',\n",
       " 'thorny',\n",
       " '46',\n",
       " 'activity',\n",
       " 'Valedictorian',\n",
       " 'evoked',\n",
       " '1950s',\n",
       " 'tomato-red',\n",
       " 'rally',\n",
       " 'mothers',\n",
       " 'tentative',\n",
       " 'dive',\n",
       " 'head-on',\n",
       " \"'51\",\n",
       " 'flood',\n",
       " 'duels',\n",
       " 'Paulah',\n",
       " 'string',\n",
       " 'ritiuality',\n",
       " 'deductions',\n",
       " 'revived',\n",
       " 'Olivia',\n",
       " 'Leukemia',\n",
       " '21st',\n",
       " 'investors',\n",
       " 'Armenian',\n",
       " 'Cadillac',\n",
       " 'Harmon',\n",
       " 'Doctor',\n",
       " 'Fireside',\n",
       " 'bites',\n",
       " 'Ankara',\n",
       " 'Bird',\n",
       " 'weaker',\n",
       " 'expected',\n",
       " 'read',\n",
       " 'economist',\n",
       " 'Sheeran',\n",
       " '37',\n",
       " 'Marty',\n",
       " 'Westphalia',\n",
       " 'metal',\n",
       " 'stagecoach',\n",
       " 'honor',\n",
       " 'oil',\n",
       " 'Ensign',\n",
       " 'heyday',\n",
       " 'Lichtenstein',\n",
       " 'sophomore',\n",
       " 'grads',\n",
       " 'radioed',\n",
       " \"Stone's\",\n",
       " '45',\n",
       " 'comradeship',\n",
       " \"Reily's\",\n",
       " 'Hammarskjold',\n",
       " 'slugged',\n",
       " 'mingle',\n",
       " 'Jody',\n",
       " 'filled',\n",
       " 'Newbold',\n",
       " 'been',\n",
       " 'induce',\n",
       " 'Lester',\n",
       " 'Luis',\n",
       " 'spell',\n",
       " 'mad',\n",
       " 'Bar',\n",
       " 'Monte',\n",
       " 'roster',\n",
       " 'saves',\n",
       " \"Massachusetts'\",\n",
       " 'firm',\n",
       " 'Sens.',\n",
       " 'qualities',\n",
       " 'probability',\n",
       " 'duel',\n",
       " 'reserving',\n",
       " 'Tommy',\n",
       " 'tax-exemption',\n",
       " 'Ex-Oriole',\n",
       " \"Rusk's\",\n",
       " \"Army's\",\n",
       " '10:45',\n",
       " 'receive',\n",
       " 'proceedings',\n",
       " 'delegate',\n",
       " 'trust',\n",
       " 'Adcock',\n",
       " 'Shirley',\n",
       " 'capitol',\n",
       " 'Happy',\n",
       " 'rescued',\n",
       " 'huddling',\n",
       " 'Gardner',\n",
       " 'tells',\n",
       " 'Drawers',\n",
       " 'Rte.',\n",
       " '$22.50',\n",
       " 'care',\n",
       " 'bluntly',\n",
       " 'varying',\n",
       " 'Reeder',\n",
       " 'Saxton',\n",
       " 'scratches',\n",
       " 'authorize',\n",
       " 'Bartha',\n",
       " 'Savage',\n",
       " 'concert',\n",
       " 'auto',\n",
       " 'reside',\n",
       " 'answer',\n",
       " 'commitment',\n",
       " 'acclimatized',\n",
       " 'Bill',\n",
       " 'jersey',\n",
       " \"general's\",\n",
       " 'majorities',\n",
       " 'Haaek',\n",
       " 'dent',\n",
       " 'Farrell',\n",
       " 'tax-free',\n",
       " 'actually',\n",
       " 'Buchanan',\n",
       " 'dishes',\n",
       " '228-229',\n",
       " 'square',\n",
       " 'Bourguiba',\n",
       " 'Command',\n",
       " 'complication',\n",
       " 'sporadic',\n",
       " 'Adams',\n",
       " 'Kerrville',\n",
       " 'marginal',\n",
       " 'Atwells',\n",
       " 'Bricker',\n",
       " 'addressing',\n",
       " 'liberals',\n",
       " \"Kramer's\",\n",
       " 'hear',\n",
       " '$80,738',\n",
       " 'chemical',\n",
       " 'Wilson',\n",
       " 'diplomacy',\n",
       " 'amicable',\n",
       " 'Kaminsky',\n",
       " 'keynote',\n",
       " 'knee',\n",
       " 'oases',\n",
       " \"She's\",\n",
       " 'Bulloch',\n",
       " 'Cancer',\n",
       " '7034',\n",
       " 'Zurcher',\n",
       " 'start',\n",
       " 'thinks',\n",
       " 'convert',\n",
       " 'nonsense',\n",
       " 'bench',\n",
       " 'mere',\n",
       " 'a',\n",
       " '1960',\n",
       " 'armed',\n",
       " \"Leavitt's\",\n",
       " \"Kunkel's\",\n",
       " '18th',\n",
       " 'prior-year',\n",
       " ')',\n",
       " 'wreck',\n",
       " 'undergoing',\n",
       " 'Worrell',\n",
       " 'excuses',\n",
       " 'inject',\n",
       " \"Women's\",\n",
       " 'parking',\n",
       " \"Emperor's\",\n",
       " 'appliques',\n",
       " 'spearhead',\n",
       " 'blackout',\n",
       " 'sphynxes',\n",
       " 'enrollments',\n",
       " 'Maple',\n",
       " 'ability',\n",
       " '1.10.1',\n",
       " 'earnest',\n",
       " 'somewhat',\n",
       " 'administrators',\n",
       " 'fuel',\n",
       " \"Fiedler's\",\n",
       " 'specialists',\n",
       " 'title',\n",
       " 'world-famous',\n",
       " 'NYU',\n",
       " 'talking',\n",
       " \"Hall's\",\n",
       " 'so-far',\n",
       " 'crabs',\n",
       " 'wrongdoing',\n",
       " 'library',\n",
       " 'staging',\n",
       " 'View',\n",
       " 'Published',\n",
       " 'feasible',\n",
       " \"D'Art\",\n",
       " 'ending',\n",
       " 'Dealer',\n",
       " 'triple',\n",
       " 'Lemon',\n",
       " 'taught',\n",
       " 'nineteenth',\n",
       " 'strikes',\n",
       " 'grab',\n",
       " 'Indications',\n",
       " 'discharged',\n",
       " 'structured',\n",
       " 'Reaction',\n",
       " 'investigate',\n",
       " '1020',\n",
       " 'crucified',\n",
       " 'underprivileged',\n",
       " 'Minoso',\n",
       " 'grain',\n",
       " 'tournament',\n",
       " 'touches',\n",
       " 'longer',\n",
       " 'Gordon',\n",
       " 'Sports',\n",
       " 'remarkably',\n",
       " 'grandchildren',\n",
       " 'frigid',\n",
       " 'pall',\n",
       " 'Millie',\n",
       " 'spoke',\n",
       " 'academic',\n",
       " 'counseling',\n",
       " 'ad',\n",
       " 'glove',\n",
       " 'switch-hitter',\n",
       " 'foreign-policy',\n",
       " 'administration',\n",
       " 'Various',\n",
       " 'majestic',\n",
       " '1899',\n",
       " 'cousin',\n",
       " 'you',\n",
       " '10-year',\n",
       " 'trials',\n",
       " 'neutralist',\n",
       " 'speculative',\n",
       " 'complementary',\n",
       " 'documentary',\n",
       " 'Montgomery',\n",
       " 'erase',\n",
       " \"Wert's\",\n",
       " 'whip',\n",
       " 'My',\n",
       " 'cook',\n",
       " 'Viceroy',\n",
       " 'Congressional',\n",
       " 'bookkeeping',\n",
       " 'Street',\n",
       " 'double',\n",
       " 'attributable',\n",
       " 'decisive',\n",
       " 'instituted',\n",
       " 'mid-flight',\n",
       " 'portions',\n",
       " '5-to-2',\n",
       " \"Escape's\",\n",
       " '124',\n",
       " 'pictured',\n",
       " 'counting',\n",
       " 'heavy',\n",
       " 'arrangement',\n",
       " 'year-to-year',\n",
       " 'AM',\n",
       " 'suburban',\n",
       " '87',\n",
       " 'Georgia',\n",
       " 'easily',\n",
       " 'die',\n",
       " 'seniors',\n",
       " 'crossed',\n",
       " 'ant',\n",
       " 'venture',\n",
       " 'grave',\n",
       " 'fines',\n",
       " 'speculated',\n",
       " 'earlier',\n",
       " 'capital',\n",
       " 'Mag',\n",
       " 'Weinstein',\n",
       " 'bride',\n",
       " 'Kemm',\n",
       " 'unconsciously',\n",
       " 'termination',\n",
       " 'licked',\n",
       " 'output',\n",
       " 'concede',\n",
       " 'Hitting',\n",
       " 'compulsory',\n",
       " 'streamliner',\n",
       " 'sleeps',\n",
       " 'Wacklin',\n",
       " 'tough',\n",
       " '70,000',\n",
       " 'Snapped',\n",
       " 'ex-schoolteacher',\n",
       " 'Palo',\n",
       " 'Salt',\n",
       " 'level',\n",
       " 'Newtown',\n",
       " 'secret',\n",
       " 'Gentile',\n",
       " 'low-wage',\n",
       " 'successor',\n",
       " 'box',\n",
       " 'All',\n",
       " 'Roosevelt',\n",
       " 'bottling',\n",
       " 'regulate',\n",
       " 'rationale',\n",
       " 'Developments',\n",
       " 'moment',\n",
       " 'Sybert',\n",
       " 'runner-up',\n",
       " 'safety',\n",
       " 'Forsyth',\n",
       " 'injunctions',\n",
       " 'properties',\n",
       " 'bowl',\n",
       " 'participation',\n",
       " 'shirking',\n",
       " 'calling',\n",
       " 'diocesan',\n",
       " 'puppet',\n",
       " 'six',\n",
       " 'Harold',\n",
       " 'sublime',\n",
       " 'stake',\n",
       " 'Strafaci',\n",
       " 'seashore',\n",
       " 'Everybody',\n",
       " 'Payson',\n",
       " 'Pamela',\n",
       " 'consider',\n",
       " 'Appeals',\n",
       " 'cerebrated',\n",
       " 'narcotic',\n",
       " 'mining',\n",
       " 'Jail',\n",
       " 'firemen',\n",
       " 'military',\n",
       " 'avidly',\n",
       " 'voting',\n",
       " 'Larson',\n",
       " 'streets',\n",
       " 'Tech',\n",
       " 'speculations',\n",
       " 'Dictator',\n",
       " '12,000',\n",
       " '33d',\n",
       " '6934',\n",
       " 'tables',\n",
       " 'specimen',\n",
       " 'earth',\n",
       " 'separating',\n",
       " 'periodic',\n",
       " 'week-end',\n",
       " 'exhibitors',\n",
       " 'cuts',\n",
       " 'exported',\n",
       " 'necessitate',\n",
       " 'canted',\n",
       " 'Lovely',\n",
       " 'Mame',\n",
       " 'exile',\n",
       " 'ruggedly',\n",
       " 'materials',\n",
       " 'Trustee',\n",
       " '3646',\n",
       " 'Fonta',\n",
       " 'moldboard',\n",
       " 'Golf',\n",
       " 'device',\n",
       " 'coveted',\n",
       " 'Dorsey',\n",
       " 'Afterward',\n",
       " 'announced',\n",
       " 'pondered',\n",
       " 'young',\n",
       " 'direction',\n",
       " 'Agencies',\n",
       " 'magistrate',\n",
       " 'proportionately',\n",
       " 'Simon',\n",
       " 'collaborators',\n",
       " 'Commission',\n",
       " 'annually',\n",
       " 'hole',\n",
       " 'putout',\n",
       " 'designs',\n",
       " 'signals',\n",
       " 'Ter.',\n",
       " 'inhomogeneous',\n",
       " 'son-in-law',\n",
       " 'connect',\n",
       " 'seven-hit',\n",
       " 'died',\n",
       " 'Mervin',\n",
       " 'loses',\n",
       " 'Belt',\n",
       " '1630',\n",
       " 'NW',\n",
       " 'operates',\n",
       " 'farewell',\n",
       " 'putter',\n",
       " 'expect',\n",
       " 'clerical-lay',\n",
       " '$17.8',\n",
       " '3.1',\n",
       " 'Clifford',\n",
       " 'reacting',\n",
       " 'undermine',\n",
       " 'power',\n",
       " 'them',\n",
       " 'Sid',\n",
       " 'calm',\n",
       " 'installed',\n",
       " 'church-state',\n",
       " 'hours',\n",
       " 'Troy',\n",
       " 'darling',\n",
       " \"Christine's\",\n",
       " 'refocusing',\n",
       " 'responding',\n",
       " 'blame',\n",
       " 'practices',\n",
       " 'Earl',\n",
       " 'Jefferson',\n",
       " 'Heritage',\n",
       " 'Beauty',\n",
       " 'Oldsmobile',\n",
       " 'its',\n",
       " 'weaken',\n",
       " \"Council's\",\n",
       " 'dozens',\n",
       " 'dominate',\n",
       " 'garde',\n",
       " 'grants',\n",
       " 'willingness',\n",
       " 'Owls',\n",
       " 'sources',\n",
       " 'ugly',\n",
       " 'Palm',\n",
       " 'blood',\n",
       " 'intensified',\n",
       " \"Here's\",\n",
       " 'ceased',\n",
       " '900',\n",
       " 'entomologist',\n",
       " 'Shortly',\n",
       " 'Ind.',\n",
       " 'supplying',\n",
       " 'whims',\n",
       " 'Delaware',\n",
       " \"Titche's\",\n",
       " 'officiated',\n",
       " 'rocket',\n",
       " 'Gould',\n",
       " 'pepper',\n",
       " 'tripled',\n",
       " 'outfield',\n",
       " 'strokes',\n",
       " 'meaning',\n",
       " 'experimental',\n",
       " 'loyalist',\n",
       " 'Station',\n",
       " 'hill',\n",
       " 'occupied',\n",
       " 'Bernet',\n",
       " 'officiate',\n",
       " 'drought-seared',\n",
       " 'Ridge',\n",
       " '8',\n",
       " 'Dukes',\n",
       " 'Herman',\n",
       " 'archaeology',\n",
       " 'Luise',\n",
       " 'exacerbated',\n",
       " 'opposition',\n",
       " 'poorly',\n",
       " 'champions',\n",
       " '26-year-old',\n",
       " 'smaller',\n",
       " 'continuous',\n",
       " 'transports',\n",
       " 'Northwestern',\n",
       " 'membership',\n",
       " 'Gill',\n",
       " 'choking',\n",
       " '900,000',\n",
       " 'Minneapolis',\n",
       " 'vice',\n",
       " 'approval',\n",
       " 'Achievement',\n",
       " 'Reyes',\n",
       " 'one-third',\n",
       " '17-1/2-inch',\n",
       " 'major-league',\n",
       " 'using',\n",
       " 'Kochaneks',\n",
       " '10',\n",
       " 'Battalion',\n",
       " 'incentive',\n",
       " 'noting',\n",
       " 'amounts',\n",
       " 'Border',\n",
       " 'centered',\n",
       " 'Washington-Oregon',\n",
       " 'Hollywood',\n",
       " 'Women',\n",
       " 'perform',\n",
       " 'pop',\n",
       " 'appoint',\n",
       " '2-hour-and-27-minute',\n",
       " 'Welsh',\n",
       " 'redoubled',\n",
       " 'Pratt',\n",
       " 'vouchers',\n",
       " '30-30',\n",
       " 'Decorating',\n",
       " 'workout',\n",
       " 'dissension',\n",
       " 'Insofar',\n",
       " 'periodicals',\n",
       " 'jury',\n",
       " 'librarians',\n",
       " 'Ku',\n",
       " '3247',\n",
       " 'outfit',\n",
       " 'War',\n",
       " 'place-kicker',\n",
       " 'escheat',\n",
       " 'composition',\n",
       " 'arithmetical',\n",
       " 'pain',\n",
       " 'April',\n",
       " 'mink',\n",
       " 'friendliness',\n",
       " 'Somerset',\n",
       " 'McKee',\n",
       " '19th',\n",
       " 'tasteful',\n",
       " 'anti-Colmer',\n",
       " 'imposing',\n",
       " 'peasant',\n",
       " 'storms',\n",
       " '$4,177.37',\n",
       " 'maiden',\n",
       " 'Leatherneck',\n",
       " 'A.A.U.',\n",
       " 'scientific',\n",
       " 'enact',\n",
       " 'Marskmen',\n",
       " 'enactment',\n",
       " 'arrived',\n",
       " 'evil',\n",
       " 'contributions',\n",
       " 'fence',\n",
       " 'prompt',\n",
       " 'jumpy',\n",
       " 'engine',\n",
       " 'newer',\n",
       " 'cooperating',\n",
       " 'Ninth',\n",
       " 'Chen',\n",
       " 'warmth',\n",
       " 'Everywhere',\n",
       " 'Arvey',\n",
       " 'assaults',\n",
       " 'surgery',\n",
       " 'white',\n",
       " 'evenings',\n",
       " 'requests',\n",
       " 'everyone',\n",
       " 'Steeves',\n",
       " 'Philip',\n",
       " 'Buck',\n",
       " 'submarine',\n",
       " 'Harvest',\n",
       " '$2,170',\n",
       " 'Charitable',\n",
       " 'Raymond',\n",
       " 'KQED',\n",
       " 'second-half',\n",
       " 'McConnell',\n",
       " 'Richey',\n",
       " 'Mellow',\n",
       " 'Peruvian',\n",
       " 'Merritt',\n",
       " 'acceptable',\n",
       " 'establishment',\n",
       " 'pockets',\n",
       " 'boarded',\n",
       " '64',\n",
       " 'faith',\n",
       " 'Cross',\n",
       " 'persistent',\n",
       " 'endure',\n",
       " 'yen',\n",
       " 'allegations',\n",
       " 'big-league',\n",
       " 'unnoticed',\n",
       " 'Pact',\n",
       " 'Barber',\n",
       " 'far',\n",
       " 'steps',\n",
       " 'trains',\n",
       " 'nursing',\n",
       " 'Salvador',\n",
       " 'Meek',\n",
       " 'prevailed',\n",
       " '12th',\n",
       " 'brochures',\n",
       " 'acclaimed',\n",
       " 'Ill.',\n",
       " 'calamity',\n",
       " 'idea',\n",
       " 'atomic',\n",
       " 'beer',\n",
       " 'spices',\n",
       " 'world-wide',\n",
       " 'Branum',\n",
       " 'Edna',\n",
       " 'graduation',\n",
       " 'winner',\n",
       " 'As',\n",
       " 'intellectually',\n",
       " '42',\n",
       " 'district',\n",
       " 'Also',\n",
       " 'offensives',\n",
       " 'continue',\n",
       " 'kickbacks',\n",
       " 'ADC',\n",
       " 'Wissahickon',\n",
       " \"women's\",\n",
       " 'impeccable',\n",
       " \"builders'\",\n",
       " 'auspiciously',\n",
       " 'Now',\n",
       " 'fell',\n",
       " 'screenings',\n",
       " 'gallery',\n",
       " 'Headquarters',\n",
       " 'It',\n",
       " 'sprained',\n",
       " 'Raoul',\n",
       " 'loudly',\n",
       " 'carpenters',\n",
       " 'confirming',\n",
       " 'forum',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numeralization\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# create to keep the all unique word in corpus on list\n",
    "vocabs = list(set(flatten(corpus))) \n",
    "# vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vocabs list to a JSON file for using it on deployment part\n",
    "with open('app/vocabs_list', 'w') as json_file1:\n",
    "    json.dump(vocabs, json_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2427"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dictionary for converting word to integer \n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n",
    "word2index['meet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add unknown to the vocabs list and convert to integer\n",
    "word2index['<UNK>'] = len(vocabs)\n",
    "vocabs.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary to a JSON file\n",
    "with open('app/word2index_dict', 'w') as json_file2:\n",
    "    json.dump(word2index, json_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'liens'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dictionary for converting integer to word\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "index2word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create random_batch function with window size = 2 to generate the pairs of center word, and outside word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        # since we assign window size = 2\n",
    "        # look from the third word until third last word\n",
    "        for i in range(2, len(doc)-2):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            #outside words = 4 words (2 words from left and 2 words from right)\n",
    "            outside = (word2index[doc[i-2]],word2index[doc[i-1]], word2index[doc[i+1]],word2index[doc[i+2]])\n",
    "            #for each of these 4 outside words, we need to append to a list\n",
    "            for each_out in outside:\n",
    "                skipgrams.append([center, each_out])\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], [] #inputs = center word, labels = outside word \n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "\n",
    "# test the random_batch function            \n",
    "x, y = random_batch(2, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9000],\n",
       "       [9969]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x #shape = (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11077],\n",
       "       [ 9598]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y  #shape = (batch_size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of each pair of words\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "skipgrams = []\n",
    "\n",
    "for doc in corpus:\n",
    "    # since we assign window size = 2\n",
    "    # look from the third word until third last word\n",
    "    for i in range(2, len(doc)-2):\n",
    "        center = doc[i]\n",
    "        outside = [doc[i-2], doc[i-1], doc[i+1], doc[i+2]]\n",
    "        for each_out in outside:\n",
    "            skipgrams.append((center, each_out))\n",
    "\n",
    "X_ik_skipgrams = Counter(skipgrams)\n",
    "# X_ik_skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weighting function to scale down of too frequent words\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    #check the pair of (w_i, w_j) is on co-occurences or not\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "\n",
    "    #if not exist, then set to 1 because the pair of words may be occured in the future\n",
    "    except:\n",
    "        x_ij = 1\n",
    "\n",
    "    #set according from GloVe paper    \n",
    "    #set xmax\n",
    "    x_max = 100\n",
    "    #set alpha\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-ocurrence does not exceeed xmax, then just multiply with some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max)**alpha\n",
    "    #otherwise, set to 1\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #dict for counting the number of pair of words\n",
    "weighting_dic = {} #scale the co-occurences using the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2): #bigram result = all possible 2 pair of words\n",
    "    if X_ik_skipgrams.get(bigram):  #if the pair exists in our corpus\n",
    "        co = X_ik_skipgrams[bigram]\n",
    "        X_ik[bigram] = co + 1 #for stability\n",
    "        X_ik[(bigram[1], bigram[0])] = co + 1 #basically is, student = student, is\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch_glove(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    random_inputs, random_labels, random_coocs, random_weightings = [], [], [], []\n",
    "    \n",
    "    #convert our skipgrams to id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly choose indexes based on batch size\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) \n",
    "    \n",
    "    #get the random input and labels\n",
    "    for index in random_index:\n",
    "        random_inputs.append([skip_grams_id[index][0]])\n",
    "        random_labels.append([skip_grams_id[index][1]])\n",
    "        #coocs\n",
    "        pair = skip_grams[index] \n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "    \n",
    "        #weightings\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [[13844]\n",
      " [ 8735]]\n",
      "y [[12735]\n",
      " [ 9598]]\n",
      "cooc [[4.99721227]\n",
      " [0.69314718]]\n",
      "weighting [[1.        ]\n",
      " [0.05318296]]\n"
     ]
    }
   ],
   "source": [
    "#test the random_batch_glove function \n",
    "batch_size = 2\n",
    "x_glove, y_glove, cooc_glove, weighting_glove = random_batch_glove(batch_size, corpus, skipgrams, X_ik, weighting_dic)\n",
    "print(\"x\",x_glove)\n",
    "print(\"y\",y_glove)\n",
    "print(\"cooc\",cooc_glove)\n",
    "print(\"weighting\",weighting_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size   = len(vocabs)\n",
    "emb_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 14392, 14393, 14394],\n",
       "        [    0,     1,     2,  ..., 14392, 14393, 14394]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocabs\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Word2Vec (Skipgram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create skipgram model\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.center_embedding(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.center_embedding(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.center_embedding(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)) # bmm is dot product (ignore batch size) and reduce dim to 2 \n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  \n",
    "        \n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (center_embedding): Embedding(14395, 2)\n",
       "  (outside_embedding): Embedding(14395, 2)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test skipgram model\n",
    "model_skipgram = Skipgram(voc_size, 2)\n",
    "model_skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_skipgram, y_skipgram = random_batch(batch_size, corpus)\n",
    "x_tensor_skipgram = torch.LongTensor(x_skipgram)\n",
    "y_tensor_skipgram = torch.LongTensor(y_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.3371, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_skipgram = model_skipgram(x_tensor_skipgram, y_tensor_skipgram, all_vocabs)\n",
    "loss_skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Word2Vec (Negative sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram distribution\n",
    "$$P(w)=U(w)^{3/4}/Z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100554"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(flatten(corpus))\n",
    "\n",
    "#count the total number of words\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign z to 0.001\n",
    "z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 114,\n",
       "         ',': 108,\n",
       "         '.': 89,\n",
       "         'of': 69,\n",
       "         'and': 55,\n",
       "         'to': 55,\n",
       "         'a': 52,\n",
       "         'in': 50,\n",
       "         'for': 30,\n",
       "         'The': 26,\n",
       "         'that': 26,\n",
       "         'was': 24,\n",
       "         \"''\": 24,\n",
       "         '``': 24,\n",
       "         'is': 24,\n",
       "         'on': 22,\n",
       "         'at': 21,\n",
       "         'with': 19,\n",
       "         'be': 19,\n",
       "         'as': 18,\n",
       "         'by': 18,\n",
       "         'he': 17,\n",
       "         'said': 15,\n",
       "         'his': 15,\n",
       "         'will': 15,\n",
       "         'it': 14,\n",
       "         'from': 14,\n",
       "         ';': 13,\n",
       "         'are': 13,\n",
       "         'an': 12,\n",
       "         'has': 12,\n",
       "         '--': 12,\n",
       "         'had': 12,\n",
       "         'not': 11,\n",
       "         'who': 11,\n",
       "         'this': 11,\n",
       "         'have': 11,\n",
       "         'Mrs.': 11,\n",
       "         'were': 11,\n",
       "         'their': 10,\n",
       "         'would': 10,\n",
       "         'which': 10,\n",
       "         'been': 9,\n",
       "         'He': 9,\n",
       "         'they': 9,\n",
       "         ')': 8,\n",
       "         'its': 8,\n",
       "         'out': 8,\n",
       "         '(': 8,\n",
       "         'one': 8,\n",
       "         'up': 8,\n",
       "         'Mr.': 8,\n",
       "         'or': 8,\n",
       "         'I': 8,\n",
       "         'more': 8,\n",
       "         'all': 8,\n",
       "         'but': 8,\n",
       "         'last': 8,\n",
       "         'about': 7,\n",
       "         'than': 7,\n",
       "         'two': 7,\n",
       "         ':': 7,\n",
       "         'first': 7,\n",
       "         'new': 7,\n",
       "         'other': 7,\n",
       "         'year': 7,\n",
       "         'A': 7,\n",
       "         'home': 6,\n",
       "         'It': 6,\n",
       "         'after': 6,\n",
       "         'In': 6,\n",
       "         'when': 6,\n",
       "         'there': 6,\n",
       "         'over': 6,\n",
       "         'also': 6,\n",
       "         'into': 6,\n",
       "         'her': 5,\n",
       "         'New': 5,\n",
       "         'them': 5,\n",
       "         'President': 5,\n",
       "         'made': 5,\n",
       "         'him': 5,\n",
       "         'week': 5,\n",
       "         'no': 5,\n",
       "         'only': 5,\n",
       "         'years': 5,\n",
       "         'can': 5,\n",
       "         'But': 5,\n",
       "         'could': 5,\n",
       "         'before': 5,\n",
       "         'time': 5,\n",
       "         'state': 5,\n",
       "         '?': 5,\n",
       "         'three': 5,\n",
       "         'some': 5,\n",
       "         'any': 5,\n",
       "         'may': 4,\n",
       "         'This': 4,\n",
       "         'against': 4,\n",
       "         'American': 4,\n",
       "         'members': 4,\n",
       "         'under': 4,\n",
       "         'work': 4,\n",
       "         'program': 4,\n",
       "         'off': 4,\n",
       "         'here': 4,\n",
       "         'such': 4,\n",
       "         'if': 4,\n",
       "         'man': 4,\n",
       "         'four': 4,\n",
       "         'night': 4,\n",
       "         'Kennedy': 4,\n",
       "         'we': 4,\n",
       "         'most': 4,\n",
       "         'so': 4,\n",
       "         'what': 4,\n",
       "         'get': 4,\n",
       "         'school': 4,\n",
       "         'now': 4,\n",
       "         'back': 4,\n",
       "         'John': 4,\n",
       "         'House': 4,\n",
       "         'administration': 3,\n",
       "         'you': 3,\n",
       "         'jury': 3,\n",
       "         'men': 3,\n",
       "         'since': 3,\n",
       "         'must': 3,\n",
       "         'Jr.': 3,\n",
       "         'then': 3,\n",
       "         'down': 3,\n",
       "         'Texas': 3,\n",
       "         'many': 3,\n",
       "         'meeting': 3,\n",
       "         'They': 3,\n",
       "         'did': 3,\n",
       "         'each': 3,\n",
       "         'tax': 3,\n",
       "         'day': 3,\n",
       "         'million': 3,\n",
       "         'Dallas': 3,\n",
       "         'these': 3,\n",
       "         'board': 3,\n",
       "         'public': 3,\n",
       "         'per': 3,\n",
       "         'system': 3,\n",
       "         'through': 3,\n",
       "         'yesterday': 3,\n",
       "         'even': 3,\n",
       "         'without': 3,\n",
       "         'plan': 3,\n",
       "         'those': 3,\n",
       "         'like': 3,\n",
       "         'City': 3,\n",
       "         \"'\": 3,\n",
       "         'game': 3,\n",
       "         'during': 3,\n",
       "         'bill': 3,\n",
       "         'should': 3,\n",
       "         'our': 3,\n",
       "         'Monday': 3,\n",
       "         'Mantle': 3,\n",
       "         'do': 3,\n",
       "         'U.S.': 3,\n",
       "         'York': 3,\n",
       "         'Miss': 3,\n",
       "         'city': 3,\n",
       "         'high': 3,\n",
       "         'Washington': 3,\n",
       "         '1': 3,\n",
       "         'where': 3,\n",
       "         'way': 3,\n",
       "         'much': 3,\n",
       "         'car': 3,\n",
       "         'State': 3,\n",
       "         'people': 3,\n",
       "         'set': 3,\n",
       "         'Sunday': 3,\n",
       "         'children': 3,\n",
       "         'take': 3,\n",
       "         'There': 3,\n",
       "         'between': 3,\n",
       "         'told': 3,\n",
       "         'being': 3,\n",
       "         'president': 3,\n",
       "         'cent': 3,\n",
       "         'government': 3,\n",
       "         'today': 3,\n",
       "         'good': 3,\n",
       "         'took': 3,\n",
       "         'because': 3,\n",
       "         'both': 3,\n",
       "         'United': 3,\n",
       "         'got': 3,\n",
       "         'sales': 3,\n",
       "         'bonds': 2,\n",
       "         'give': 2,\n",
       "         'B.': 2,\n",
       "         '!': 2,\n",
       "         'runs': 2,\n",
       "         'May': 2,\n",
       "         'until': 2,\n",
       "         'expected': 2,\n",
       "         'firm': 2,\n",
       "         'start': 2,\n",
       "         '1960': 2,\n",
       "         'library': 2,\n",
       "         'military': 2,\n",
       "         'announced': 2,\n",
       "         'young': 2,\n",
       "         '10': 2,\n",
       "         'far': 2,\n",
       "         'As': 2,\n",
       "         'political': 2,\n",
       "         'chairman': 2,\n",
       "         'several': 2,\n",
       "         'early': 2,\n",
       "         'States': 2,\n",
       "         'taken': 2,\n",
       "         'second': 2,\n",
       "         'University': 2,\n",
       "         'laws': 2,\n",
       "         'hit': 2,\n",
       "         'see': 2,\n",
       "         'former': 2,\n",
       "         'university': 2,\n",
       "         'special': 2,\n",
       "         'third': 2,\n",
       "         'White': 2,\n",
       "         'own': 2,\n",
       "         'At': 2,\n",
       "         'meet': 2,\n",
       "         'Player': 2,\n",
       "         'big': 2,\n",
       "         'North': 2,\n",
       "         'called': 2,\n",
       "         'county': 2,\n",
       "         'make': 2,\n",
       "         'received': 2,\n",
       "         'end': 2,\n",
       "         'better': 2,\n",
       "         'Court': 2,\n",
       "         'Maris': 2,\n",
       "         'company': 2,\n",
       "         'That': 2,\n",
       "         'come': 2,\n",
       "         'Richard': 2,\n",
       "         'think': 2,\n",
       "         'right': 2,\n",
       "         'Robert': 2,\n",
       "         'use': 2,\n",
       "         'And': 2,\n",
       "         'just': 2,\n",
       "         'number': 2,\n",
       "         'reported': 2,\n",
       "         'run': 2,\n",
       "         'Senate': 2,\n",
       "         'problem': 2,\n",
       "         'total': 2,\n",
       "         'area': 2,\n",
       "         'my': 2,\n",
       "         'later': 2,\n",
       "         'J.': 2,\n",
       "         'house': 2,\n",
       "         'member': 2,\n",
       "         'Palmer': 2,\n",
       "         'does': 2,\n",
       "         'pay': 2,\n",
       "         'asked': 2,\n",
       "         'ever': 2,\n",
       "         'Soviet': 2,\n",
       "         'another': 2,\n",
       "         'Tuesday': 2,\n",
       "         'too': 2,\n",
       "         'eight': 2,\n",
       "         'however': 2,\n",
       "         'ball': 2,\n",
       "         'Club': 2,\n",
       "         'St.': 2,\n",
       "         'James': 2,\n",
       "         'equipment': 2,\n",
       "         'possible': 2,\n",
       "         'go': 2,\n",
       "         'center': 2,\n",
       "         'daughter': 2,\n",
       "         'added': 2,\n",
       "         'best': 2,\n",
       "         'given': 2,\n",
       "         'never': 2,\n",
       "         'together': 2,\n",
       "         'Saturday': 2,\n",
       "         'me': 2,\n",
       "         'months': 2,\n",
       "         'season': 2,\n",
       "         'H.': 2,\n",
       "         'small': 2,\n",
       "         '&': 2,\n",
       "         'project': 2,\n",
       "         'service': 2,\n",
       "         'open': 2,\n",
       "         'present': 2,\n",
       "         'club': 2,\n",
       "         'business': 2,\n",
       "         'local': 2,\n",
       "         'show': 2,\n",
       "         'federal': 2,\n",
       "         'We': 2,\n",
       "         'Committee': 2,\n",
       "         'When': 2,\n",
       "         'She': 2,\n",
       "         'W.': 2,\n",
       "         'while': 2,\n",
       "         'higher': 2,\n",
       "         'law': 2,\n",
       "         'result': 2,\n",
       "         'major': 2,\n",
       "         'His': 2,\n",
       "         'went': 2,\n",
       "         'party': 2,\n",
       "         'group': 2,\n",
       "         'To': 2,\n",
       "         'held': 2,\n",
       "         'past': 2,\n",
       "         'need': 2,\n",
       "         'industry': 2,\n",
       "         'again': 2,\n",
       "         'found': 2,\n",
       "         'began': 2,\n",
       "         'March': 2,\n",
       "         'put': 2,\n",
       "         'great': 2,\n",
       "         'Friday': 2,\n",
       "         'Judge': 2,\n",
       "         'foreign': 2,\n",
       "         'National': 2,\n",
       "         'For': 2,\n",
       "         'long': 2,\n",
       "         'family': 2,\n",
       "         'next': 2,\n",
       "         'C.': 2,\n",
       "         'Democratic': 2,\n",
       "         'County': 2,\n",
       "         'five': 2,\n",
       "         'police': 2,\n",
       "         'vote': 2,\n",
       "         'director': 2,\n",
       "         'Dr.': 2,\n",
       "         'ago': 2,\n",
       "         'part': 2,\n",
       "         'around': 2,\n",
       "         'interest': 2,\n",
       "         'world': 2,\n",
       "         'top': 2,\n",
       "         'schools': 2,\n",
       "         'Communist': 2,\n",
       "         'late': 2,\n",
       "         'enough': 2,\n",
       "         'she': 2,\n",
       "         'p.m.': 2,\n",
       "         'very': 2,\n",
       "         'committee': 2,\n",
       "         'E.': 2,\n",
       "         'If': 2,\n",
       "         'left': 2,\n",
       "         'staff': 2,\n",
       "         'few': 2,\n",
       "         'annual': 2,\n",
       "         'might': 2,\n",
       "         'session': 2,\n",
       "         'general': 2,\n",
       "         'William': 2,\n",
       "         'Co.': 2,\n",
       "         'little': 2,\n",
       "         'needed': 2,\n",
       "         'month': 2,\n",
       "         'One': 2,\n",
       "         'days': 2,\n",
       "         'wife': 2,\n",
       "         'market': 2,\n",
       "         'election': 2,\n",
       "         'team': 2,\n",
       "         'S.': 2,\n",
       "         'record': 2,\n",
       "         'case': 2,\n",
       "         'says': 2,\n",
       "         'how': 2,\n",
       "         'issue': 2,\n",
       "         'well': 2,\n",
       "         '1961': 2,\n",
       "         'same': 2,\n",
       "         'education': 2,\n",
       "         'court': 2,\n",
       "         'countries': 2,\n",
       "         'Laos': 2,\n",
       "         'still': 2,\n",
       "         '2': 2,\n",
       "         'national': 2,\n",
       "         'came': 2,\n",
       "         'Republican': 2,\n",
       "         'know': 2,\n",
       "         'Congo': 2,\n",
       "         'play': 2,\n",
       "         'along': 2,\n",
       "         'help': 2,\n",
       "         '4': 2,\n",
       "         'funds': 2,\n",
       "         'money': 2,\n",
       "         'On': 2,\n",
       "         'A.': 2,\n",
       "         'followed': 1,\n",
       "         'father': 1,\n",
       "         'study': 1,\n",
       "         'central': 1,\n",
       "         'society': 1,\n",
       "         'students': 1,\n",
       "         'coal': 1,\n",
       "         'entire': 1,\n",
       "         'believe': 1,\n",
       "         'prevent': 1,\n",
       "         'audience': 1,\n",
       "         'Their': 1,\n",
       "         'firms': 1,\n",
       "         'growth': 1,\n",
       "         'felt': 1,\n",
       "         'already': 1,\n",
       "         'read': 1,\n",
       "         'honor': 1,\n",
       "         'receive': 1,\n",
       "         'care': 1,\n",
       "         'concert': 1,\n",
       "         'Bill': 1,\n",
       "         'hear': 1,\n",
       "         'knee': 1,\n",
       "         'tournament': 1,\n",
       "         'Street': 1,\n",
       "         'double': 1,\n",
       "         'Georgia': 1,\n",
       "         'earlier': 1,\n",
       "         'level': 1,\n",
       "         'All': 1,\n",
       "         'six': 1,\n",
       "         'hole': 1,\n",
       "         'power': 1,\n",
       "         'hours': 1,\n",
       "         'grants': 1,\n",
       "         '8': 1,\n",
       "         'vice': 1,\n",
       "         'using': 1,\n",
       "         'April': 1,\n",
       "         'white': 1,\n",
       "         'district': 1,\n",
       "         'continue': 1,\n",
       "         'Now': 1,\n",
       "         'collection': 1,\n",
       "         'pitching': 1,\n",
       "         'want': 1,\n",
       "         'private': 1,\n",
       "         'close': 1,\n",
       "         'am': 1,\n",
       "         'action': 1,\n",
       "         'billion': 1,\n",
       "         'owners': 1,\n",
       "         'working': 1,\n",
       "         '3': 1,\n",
       "         'really': 1,\n",
       "         'Church': 1,\n",
       "         'dropped': 1,\n",
       "         'battle': 1,\n",
       "         'according': 1,\n",
       "         'played': 1,\n",
       "         '1958': 1,\n",
       "         'women': 1,\n",
       "         'round': 1,\n",
       "         'short': 1,\n",
       "         'perhaps': 1,\n",
       "         'nations': 1,\n",
       "         'Louis': 1,\n",
       "         'bargaining': 1,\n",
       "         'July': 1,\n",
       "         'clear': 1,\n",
       "         'El': 1,\n",
       "         'General': 1,\n",
       "         'Sam': 1,\n",
       "         'date': 1,\n",
       "         'hard': 1,\n",
       "         'spirit': 1,\n",
       "         'thought': 1,\n",
       "         'history': 1,\n",
       "         'George': 1,\n",
       "         'student': 1,\n",
       "         'especially': 1,\n",
       "         'campaign': 1,\n",
       "         'book': 1,\n",
       "         \"it's\": 1,\n",
       "         'Air': 1,\n",
       "         'food': 1,\n",
       "         'thing': 1,\n",
       "         'concerned': 1,\n",
       "         '7': 1,\n",
       "         'Park': 1,\n",
       "         'Chicago': 1,\n",
       "         'coach': 1,\n",
       "         'areas': 1,\n",
       "         'shot': 1,\n",
       "         'son': 1,\n",
       "         'evening': 1,\n",
       "         'yards': 1,\n",
       "         'fees': 1,\n",
       "         'including': 1,\n",
       "         'reasons': 1,\n",
       "         'labor': 1,\n",
       "         'loss': 1,\n",
       "         'judge': 1,\n",
       "         'pass': 1,\n",
       "         'price': 1,\n",
       "         'real': 1,\n",
       "         'shown': 1,\n",
       "         'makes': 1,\n",
       "         'revenues': 1,\n",
       "         'Center': 1,\n",
       "         'ruled': 1,\n",
       "         'governor': 1,\n",
       "         'players': 1,\n",
       "         'worth': 1,\n",
       "         '30': 1,\n",
       "         'free': 1,\n",
       "         \"doesn't\": 1,\n",
       "         'measure': 1,\n",
       "         'half': 1,\n",
       "         'U.N.': 1,\n",
       "         'libraries': 1,\n",
       "         'tell': 1,\n",
       "         'cars': 1,\n",
       "         'David': 1,\n",
       "         'television': 1,\n",
       "         'line': 1,\n",
       "         'address': 1,\n",
       "         'Both': 1,\n",
       "         'example': 1,\n",
       "         'returned': 1,\n",
       "         'persons': 1,\n",
       "         'knew': 1,\n",
       "         'provide': 1,\n",
       "         'G.': 1,\n",
       "         'Wagner': 1,\n",
       "         'stand': 1,\n",
       "         'cannot': 1,\n",
       "         'kept': 1,\n",
       "         'What': 1,\n",
       "         'brought': 1,\n",
       "         'getting': 1,\n",
       "         'Republicans': 1,\n",
       "         'passing': 1,\n",
       "         'operation': 1,\n",
       "         'increase': 1,\n",
       "         'complete': 1,\n",
       "         'Austin': 1,\n",
       "         'farm': 1,\n",
       "         'afternoon': 1,\n",
       "         'development': 1,\n",
       "         'Association': 1,\n",
       "         'Government': 1,\n",
       "         'event': 1,\n",
       "         'himself': 1,\n",
       "         'attorney': 1,\n",
       "         'done': 1,\n",
       "         'admitted': 1,\n",
       "         'league': 1,\n",
       "         'contract': 1,\n",
       "         'whole': 1,\n",
       "         'hardly': 1,\n",
       "         'candidate': 1,\n",
       "         'construction': 1,\n",
       "         'organization': 1,\n",
       "         'proposal': 1,\n",
       "         'Warren': 1,\n",
       "         'needs': 1,\n",
       "         'spent': 1,\n",
       "         'costs': 1,\n",
       "         'International': 1,\n",
       "         'won': 1,\n",
       "         'caused': 1,\n",
       "         'effect': 1,\n",
       "         'Day': 1,\n",
       "         'N.': 1,\n",
       "         'Moritz': 1,\n",
       "         'AP': 1,\n",
       "         'Geneva': 1,\n",
       "         'interested': 1,\n",
       "         'seem': 1,\n",
       "         'friends': 1,\n",
       "         '20': 1,\n",
       "         'include': 1,\n",
       "         'urged': 1,\n",
       "         'forces': 1,\n",
       "         'international': 1,\n",
       "         'South': 1,\n",
       "         'water': 1,\n",
       "         'something': 1,\n",
       "         '9': 1,\n",
       "         'bring': 1,\n",
       "         'About': 1,\n",
       "         'Belgians': 1,\n",
       "         'became': 1,\n",
       "         'nuclear': 1,\n",
       "         'Mickey': 1,\n",
       "         'report': 1,\n",
       "         'view': 1,\n",
       "         'lives': 1,\n",
       "         'race': 1,\n",
       "         'war': 1,\n",
       "         'recent': 1,\n",
       "         'office': 1,\n",
       "         'Lawrence': 1,\n",
       "         'elected': 1,\n",
       "         'Each': 1,\n",
       "         'research': 1,\n",
       "         'straight': 1,\n",
       "         'rules': 1,\n",
       "         'hope': 1,\n",
       "         'Fulton': 1,\n",
       "         '17': 1,\n",
       "         'Hill': 1,\n",
       "         'states': 1,\n",
       "         'employees': 1,\n",
       "         'fact': 1,\n",
       "         'coming': 1,\n",
       "         '25': 1,\n",
       "         'though': 1,\n",
       "         'feet': 1,\n",
       "         'sense': 1,\n",
       "         'headed': 1,\n",
       "         'current': 1,\n",
       "         'Thursday': 1,\n",
       "         'plus': 1,\n",
       "         'going': 1,\n",
       "         '15': 1,\n",
       "         'course': 1,\n",
       "         'apparently': 1,\n",
       "         'wanted': 1,\n",
       "         'above': 1,\n",
       "         'junior': 1,\n",
       "         'Howard': 1,\n",
       "         'M.': 1,\n",
       "         \"didn't\": 1,\n",
       "         'opening': 1,\n",
       "         'probably': 1,\n",
       "         'teaching': 1,\n",
       "         'important': 1,\n",
       "         'near': 1,\n",
       "         'possibility': 1,\n",
       "         'personnel': 1,\n",
       "         'rather': 1,\n",
       "         'necessary': 1,\n",
       "         'Eisenhower': 1,\n",
       "         'individual': 1,\n",
       "         'Hotel': 1,\n",
       "         'become': 1,\n",
       "         'Yankees': 1,\n",
       "         'among': 1,\n",
       "         'side': 1,\n",
       "         'charge': 1,\n",
       "         'old': 1,\n",
       "         'led': 1,\n",
       "         'Feb.': 1,\n",
       "         'throughout': 1,\n",
       "         'Federal': 1,\n",
       "         'Denver': 1,\n",
       "         'field': 1,\n",
       "         'rule': 1,\n",
       "         'League': 1,\n",
       "         'playing': 1,\n",
       "         'Houston': 1,\n",
       "         'prices': 1,\n",
       "         \"President's\": 1,\n",
       "         'medical': 1,\n",
       "         'Angeles': 1,\n",
       "         'security': 1,\n",
       "         'College': 1,\n",
       "         'point': 1,\n",
       "         'civil': 1,\n",
       "         'Secretary': 1,\n",
       "         'developed': 1,\n",
       "         'walk': 1,\n",
       "         'either': 1,\n",
       "         'base': 1,\n",
       "         'Premier': 1,\n",
       "         'Mitchell': 1,\n",
       "         'driven': 1,\n",
       "         'failed': 1,\n",
       "         'Army': 1,\n",
       "         'lines': 1,\n",
       "         'running': 1,\n",
       "         'final': 1,\n",
       "         '5': 1,\n",
       "         'move': 1,\n",
       "         'ready': 1,\n",
       "         'couple': 1,\n",
       "         'future': 1,\n",
       "         'Emory': 1,\n",
       "         'Jim': 1,\n",
       "         'particularly': 1,\n",
       "         'boy': 1,\n",
       "         'secretary': 1,\n",
       "         'Oct.': 1,\n",
       "         'attack': 1,\n",
       "         'East': 1,\n",
       "         'presented': 1,\n",
       "         'whose': 1,\n",
       "         'name': 1,\n",
       "         'effective': 1,\n",
       "         'considered': 1,\n",
       "         'personal': 1,\n",
       "         'making': 1,\n",
       "         'generally': 1,\n",
       "         'worked': 1,\n",
       "         'guests': 1,\n",
       "         'After': 1,\n",
       "         'School': 1,\n",
       "         'De': 1,\n",
       "         'news': 1,\n",
       "         'color': 1,\n",
       "         'floor': 1,\n",
       "         'declared': 1,\n",
       "         'arrested': 1,\n",
       "         'theater': 1,\n",
       "         'certain': 1,\n",
       "         'An': 1,\n",
       "         'No': 1,\n",
       "         'previous': 1,\n",
       "         'indicated': 1,\n",
       "         'bills': 1,\n",
       "         'pressure': 1,\n",
       "         '6': 1,\n",
       "         'addition': 1,\n",
       "         'return': 1,\n",
       "         'struck': 1,\n",
       "         'place': 1,\n",
       "         'fight': 1,\n",
       "         'amount': 1,\n",
       "         'evidence': 1,\n",
       "         'Service': 1,\n",
       "         'expansion': 1,\n",
       "         'build': 1,\n",
       "         'joined': 1,\n",
       "         'products': 1,\n",
       "         'negotiations': 1,\n",
       "         'moved': 1,\n",
       "         'question': 1,\n",
       "         'speed': 1,\n",
       "         'official': 1,\n",
       "         'daily': 1,\n",
       "         'anti-trust': 1,\n",
       "         'large': 1,\n",
       "         'trial': 1,\n",
       "         'executive': 1,\n",
       "         'position': 1,\n",
       "         'proposed': 1,\n",
       "         'bus': 1,\n",
       "         'favor': 1,\n",
       "         'away': 1,\n",
       "         '1959': 1,\n",
       "         'First': 1,\n",
       "         'effort': 1,\n",
       "         'games': 1,\n",
       "         'army': 1,\n",
       "         'These': 1,\n",
       "         'gave': 1,\n",
       "         'textile': 1,\n",
       "         'seven': 1,\n",
       "         'within': 1,\n",
       "         'secrets': 1,\n",
       "         'Union': 1,\n",
       "         'Of': 1,\n",
       "         'Portland': 1,\n",
       "         'similar': 1,\n",
       "         'Smith': 1,\n",
       "         'Wednesday': 1,\n",
       "         'seek': 1,\n",
       "         'least': 1,\n",
       "         'Orleans': 1,\n",
       "         'kind': 1,\n",
       "         'hospital': 1,\n",
       "         '11': 1,\n",
       "         'tomorrow': 1,\n",
       "         'Some': 1,\n",
       "         'You': 1,\n",
       "         'full': 1,\n",
       "         'job': 1,\n",
       "         'across': 1,\n",
       "         'Legislature': 1,\n",
       "         'following': 1,\n",
       "         'taking': 1,\n",
       "         'commission': 1,\n",
       "         'husband': 1,\n",
       "         'religious': 1,\n",
       "         'driving': 1,\n",
       "         'talk': 1,\n",
       "         'series': 1,\n",
       "         'manager': 1,\n",
       "         'Education': 1,\n",
       "         'estimated': 1,\n",
       "         'your': 1,\n",
       "         'Jones': 1,\n",
       "         'Mayor': 1,\n",
       "         'matter': 1,\n",
       "         'High': 1,\n",
       "         'toward': 1,\n",
       "         'designed': 1,\n",
       "         'explained': 1,\n",
       "         'art': 1,\n",
       "         'basis': 1,\n",
       "         'nearly': 1,\n",
       "         'head': 1,\n",
       "         'key': 1,\n",
       "         'However': 1,\n",
       "         'property': 1,\n",
       "         'period': 1,\n",
       "         'letters': 1,\n",
       "         'college': 1,\n",
       "         'leading': 1,\n",
       "         'spring': 1,\n",
       "         'average': 1,\n",
       "         'nation': 1,\n",
       "         'minutes': 1,\n",
       "         'act': 1,\n",
       "         'term': 1,\n",
       "         'often': 1,\n",
       "         '12': 1,\n",
       "         'age': 1,\n",
       "         'further': 1,\n",
       "         'fund': 1,\n",
       "         'summer': 1,\n",
       "         'population': 1,\n",
       "         'follow': 1,\n",
       "         'salary': 1,\n",
       "         'officials': 1,\n",
       "         'plane': 1,\n",
       "         'social': 1,\n",
       "         'Thomas': 1,\n",
       "         '60': 1,\n",
       "         'machinery': 1,\n",
       "         'Many': 1,\n",
       "         'Island': 1,\n",
       "         'Rev.': 1,\n",
       "         'neither': 1,\n",
       "         'faculty': 1,\n",
       "         'grand': 1,\n",
       "         'June': 1,\n",
       "         \"It's\": 1,\n",
       "         'groups': 1,\n",
       "         'Pittsburgh': 1,\n",
       "         'programs': 1,\n",
       "         'turn': 1,\n",
       "         'clearly': 1,\n",
       "         'merely': 1,\n",
       "         'themselves': 1,\n",
       "         \"year's\": 1,\n",
       "         'legislation': 1,\n",
       "         'Central': 1,\n",
       "         'Nations': 1,\n",
       "         'serious': 1,\n",
       "         'situation': 1,\n",
       "         'word': 1,\n",
       "         'nine': 1,\n",
       "         'started': 1,\n",
       "         '100': 1,\n",
       "         'Blue': 1,\n",
       "         'Joseph': 1,\n",
       "         'P.': 1,\n",
       "         'Khrushchev': 1,\n",
       "         'voters': 1,\n",
       "         'So': 1,\n",
       "         'face': 1,\n",
       "         'decision': 1,\n",
       "         'support': 1,\n",
       "         'hits': 1,\n",
       "         'drive': 1,\n",
       "         'Force': 1,\n",
       "         'community': 1,\n",
       "         'town': 1,\n",
       "         'cost': 1,\n",
       "         'union': 1,\n",
       "         'control': 1,\n",
       "         'tried': 1,\n",
       "         'workers': 1,\n",
       "         'paid': 1,\n",
       "         'call': 1,\n",
       "         'fire': 1,\n",
       "         'times': 1,\n",
       "         'conference': 1,\n",
       "         'award': 1,\n",
       "         \"can't\": 1,\n",
       "         'aid': 1,\n",
       "         'Among': 1,\n",
       "         'San': 1,\n",
       "         'known': 1,\n",
       "         'land': 1,\n",
       "         'D.': 1,\n",
       "         '14': 1,\n",
       "         'continued': 1,\n",
       "         'charged': 1,\n",
       "         'share': 1,\n",
       "         'passed': 1,\n",
       "         'financial': 1,\n",
       "         'Christian': 1,\n",
       "         '200': 1,\n",
       "         'weeks': 1,\n",
       "         'counties': 1,\n",
       "         'attend': 1,\n",
       "         'primary': 1,\n",
       "         'Aj': 1,\n",
       "         'traffic': 1,\n",
       "         'chief': 1,\n",
       "         'officers': 1,\n",
       "         'companies': 1,\n",
       "         'leaders': 1,\n",
       "         'relations': 1,\n",
       "         'us': 1,\n",
       "         'conspiracy': 1,\n",
       "         'Jack': 1,\n",
       "         'condition': 1,\n",
       "         'church': 1,\n",
       "         \"Kennedy's\": 1,\n",
       "         'West': 1,\n",
       "         'leader': 1,\n",
       "         'council': 1,\n",
       "         '18': 1,\n",
       "         'Council': 1,\n",
       "         'Jan.': 1,\n",
       "         'scheduled': 1,\n",
       "         'front': 1,\n",
       "         'policy': 1,\n",
       "         'agreed': 1,\n",
       "         'verdict': 1,\n",
       "         'dinner': 1,\n",
       "         'beat': 1,\n",
       "         'hand': 1,\n",
       "         'road': 1,\n",
       "         'almost': 1,\n",
       "         'Russia': 1,\n",
       "         'Moscow': 1,\n",
       "         'Old': 1,\n",
       "         'Havana': 1,\n",
       "         'a.m.': 1,\n",
       "         'live': 1,\n",
       "         'having': 1,\n",
       "         'weekend': 1,\n",
       "         'Family': 1,\n",
       "         'calls': 1,\n",
       "         'Los': 1,\n",
       "         'finished': 1,\n",
       "         'find': 1,\n",
       "         'trade': 1,\n",
       "         'U.': 1,\n",
       "         'building': 1,\n",
       "         'golf': 1,\n",
       "         'opinion': 1,\n",
       "         'score': 1,\n",
       "         'hearing': 1,\n",
       "         'gin': 1,\n",
       "         'L.': 1,\n",
       "         'order': 1,\n",
       "         'lead': 1,\n",
       "         'churches': 1,\n",
       "         'whether': 1,\n",
       "         'others': 1,\n",
       "         'headquarters': 1,\n",
       "         'involved': 1,\n",
       "         'Democrats': 1,\n",
       "         'Hospital': 1,\n",
       "         'Last': 1,\n",
       "         'fine': 1,\n",
       "         'Americans': 1,\n",
       "         'able': 1,\n",
       "         'housing': 1,\n",
       "         'decided': 1,\n",
       "         'despite': 1,\n",
       "         'Atlanta': 1,\n",
       "         'keep': 1,\n",
       "         'Providence': 1,\n",
       "         'independence': 1,\n",
       "         'Cuba': 1,\n",
       "         'used': 1,\n",
       "         'Board': 1,\n",
       "         'More': 1,\n",
       "         'opportunity': 1,\n",
       "         'reached': 1,\n",
       "         'Bob': 1,\n",
       "         'Corps': 1,\n",
       "         'required': 1,\n",
       "         '22': 1,\n",
       "         'Hughes': 1,\n",
       "         'income': 1,\n",
       "         'plans': 1,\n",
       "         'demand': 1,\n",
       "         'charter': 1,\n",
       "         'Hillsboro': 1,\n",
       "         'Boston': 1,\n",
       "         'Sen.': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for v in vocabs:\n",
    "    uw = word_count[v] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([v] * uw_alpha)\n",
    "    \n",
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# sample 5 words on corpus\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):  #(1, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample      = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "        \n",
    "    return torch.cat(neg_samples) #batch_size, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_neg, y_neg = random_batch(batch_size, corpus)\n",
    "x_tensor_neg = torch.LongTensor(x_neg)\n",
    "y_tensor_neg = torch.LongTensor(y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "# test negative_sampling\n",
    "neg_samples = negative_sampling(y_tensor_neg, unigram_table, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9143])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor_neg[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6946, 14111, 11170,  5759,  4408])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create skipgram negative sampling\n",
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.center_embedding(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.outside_embedding(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.outside_embedding(negative) #(bs, k, emb_size)\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1) #sum on second dim\n",
    "        \n",
    "        # calculate loss\n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your model\n",
    "model_test_neg = SkipgramNeg(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7951, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_neg = model_test_neg(x_tensor_neg, y_tensor_neg, neg_samples)\n",
    "loss_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create glove model\n",
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        center_embeds  = self.center_embedding(center) #(batch_size, 1, emb_size)\n",
    "        outside_embeds = self.outside_embedding(outside) #(batch_size, 1, emb_size)\n",
    "        \n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test glove model\n",
    "model_glove = Glove(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensor\n",
    "x_tensor_glove = torch.LongTensor(x_glove)\n",
    "y_tensor_glove = torch.LongTensor(y_glove)\n",
    "cooc_tensor_glove = torch.FloatTensor(cooc_glove)\n",
    "weighting_tensor_glove = torch.FloatTensor(weighting_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(45.3896, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print loss\n",
    "loss_glove = model_glove(x_tensor_glove, y_tensor_glove, cooc_tensor_glove, weighting_tensor_glove)\n",
    "loss_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating the training time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Word2Vec (Skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram      = Skipgram(voc_size, emb_size)\n",
    "optimizer_skipgram  = optim.Adam(model_skipgram.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    200 | Loss: 9.322708\n",
      "Epoch    400 | Loss: 10.832323\n",
      "Epoch    600 | Loss: 10.628942\n",
      "Epoch    800 | Loss: 10.641950\n",
      "Epoch   1000 | Loss: 9.970019\n",
      "Epoch   1200 | Loss: 10.455753\n",
      "Epoch   1400 | Loss: 9.368336\n",
      "Epoch   1600 | Loss: 10.596554\n",
      "Epoch   1800 | Loss: 9.585976\n",
      "Epoch   2000 | Loss: 10.171299\n",
      "time: 15m 56s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 2000\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch_skipgram, label_batch_skipgram = random_batch(batch_size, corpus)\n",
    "    input_tensor_skipgram = torch.LongTensor(input_batch_skipgram)\n",
    "    label_tensor_skipgram = torch.LongTensor(label_batch_skipgram)\n",
    "     \n",
    "    #predict\n",
    "    loss_skipgram = model_skipgram(input_tensor_skipgram, label_tensor_skipgram, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer_skipgram.zero_grad()\n",
    "    loss_skipgram.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer_skipgram.step()\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_skipgram:2.6f}\") #Epoch 6 front space, 0 back space\n",
    "\n",
    "end = time.time()\n",
    "epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "print(f\"time: {epoch_mins}m {epoch_secs}s\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the skipgram model\n",
    "torch.save(model_skipgram, 'skipgram_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Word2Vec (Negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neg     = SkipgramNeg(voc_size, emb_size)\n",
    "optimizer_neg = optim.Adam(model_neg.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    200 | Loss: 7.423603\n",
      "Epoch    400 | Loss: 6.577616\n",
      "Epoch    600 | Loss: 0.897706\n",
      "Epoch    800 | Loss: 3.228155\n",
      "Epoch   1000 | Loss: 1.562261\n",
      "Epoch   1200 | Loss: 1.186448\n",
      "Epoch   1400 | Loss: 0.796024\n",
      "Epoch   1600 | Loss: 2.623813\n",
      "Epoch   1800 | Loss: 1.790620\n",
      "Epoch   2000 | Loss: 2.019679\n",
      "time: 15m 52s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 2000\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch_neg, label_batch_neg = random_batch(batch_size, corpus)\n",
    "    input_tensor_neg = torch.LongTensor(input_batch_neg)\n",
    "    label_tensor_neg = torch.LongTensor(label_batch_neg)\n",
    "    \n",
    "    #predict\n",
    "    neg_samples = negative_sampling(label_tensor_neg, unigram_table, k)\n",
    "    loss_neg = model_neg(input_tensor_neg, label_tensor_neg, neg_samples)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer_neg.zero_grad()\n",
    "    loss_neg.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer_neg.step()\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_neg:2.6f}\")\n",
    "\n",
    "end = time.time()\n",
    "epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "print(f\"time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the skipgram negative sampling model\n",
    "torch.save(model_neg, 'skipgramNEG_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = Glove(voc_size, emb_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_glove = optim.Adam(model_glove.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    200 | Loss: 37.209587\n",
      "Epoch:    400 | Loss: 10.090467\n",
      "Epoch:    600 | Loss: 41.918808\n",
      "Epoch:    800 | Loss: 1.139659\n",
      "Epoch:   1000 | Loss: 0.094033\n",
      "Epoch:   1200 | Loss: 1.894140\n",
      "Epoch:   1400 | Loss: 0.620077\n",
      "Epoch:   1600 | Loss: 3.511661\n",
      "Epoch:   1800 | Loss: 0.629111\n",
      "Epoch:   2000 | Loss: 0.557695\n",
      "time: 2m 19s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 2000\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "        \n",
    "    input_batch_glove, target_batch_glove, cooc_batch_glove, weighting_batch_glove = random_batch_glove(batch_size, corpus, skipgrams, X_ik, weighting_dic)\n",
    "    input_batch_glove  = torch.LongTensor(input_batch_glove)         #[batch_size, 1]\n",
    "    target_batch_glove = torch.LongTensor(target_batch_glove)        #[batch_size, 1]\n",
    "    cooc_batch_glove   = torch.FloatTensor(cooc_batch_glove)         #[batch_size, 1]\n",
    "    weighting_batch_glove = torch.FloatTensor(weighting_batch_glove) #[batch_size, 1]\n",
    "    \n",
    "    optimizer_glove.zero_grad()\n",
    "    loss_glove = model_glove(input_batch_glove, target_batch_glove, cooc_batch_glove, weighting_batch_glove)\n",
    "    \n",
    "    loss_glove.backward()\n",
    "    optimizer_glove.step()\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch: {epoch + 1:6.0f} | Loss: {loss_glove:2.6f}\")\n",
    "\n",
    "end = time.time()\n",
    "epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "print(f\"time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_glove, 'glove_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task2.1 Compare training loss and training time of Skip-gram, Skip-gram negative sampling, and GloVe models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model     | training loss @ epoch 2000 | training time| \n",
    "|:----------|:---------------------------:|:-------------:|\n",
    "| Skipgram  |       10.171299             |    15m 56s     |  \n",
    "| Skipgram (NEG) |  2.019679              |    15m 52s    | \n",
    "| Glove     |       0.557695            |    2m 19s     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task2.2 Compare and calculatesyntactic and semantic accuracy of Skip-gram, Skip-gram negative sampling, GloVe , and GloVe(Gensim) models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_data = []\n",
    "with open('data/capital-common-countries.txt') as file1:\n",
    "    for line in file1:\n",
    "        semantic_data.append(line.split())\n",
    "# semantic_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntactic_data = []\n",
    "with open('data/past-tense.txt') as file2:\n",
    "    for line in file2:\n",
    "        syntactic_data.append(line.split())\n",
    "# syntactic_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(model,x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_analogy(model,data):\n",
    "    count = 0\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            word = analogy(model,data[i][0],data[i][1] , data[i][2])\n",
    "            if word == data[i][3]:\n",
    "                count += 1\n",
    "        except:\n",
    "            continue\n",
    "    return count/len(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved glove model\n",
    "\n",
    "loaded_skipgram_model=torch.load('skipgram_model.pth')\n",
    "loaded_skipgram_model.eval()  \n",
    "\n",
    "#extract the word vectors from the loaded model\n",
    "word_vectors_skipgram = loaded_skipgram_model.center_embedding.weight.data.numpy()\n",
    "word_vectors_skipgram\n",
    "#save the word vectors \n",
    "np.savetxt('skipgram_word_vectors.txt', word_vectors_skipgram, delimiter=' ')\n",
    "\n",
    "#add word along with word vectors\n",
    "with open('skipgram_word_vectors.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, word in enumerate(vocabs):\n",
    "        vector_skipgram = \" \".join(str(value) for value in word_vectors_skipgram[i])\n",
    "        f.write(f\"{word} {vector_skipgram}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model = KeyedVectors.load_word2vec_format('skipgram_word_vectors.txt', binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_analogy(skipgram_model,semantic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_analogy(skipgram_model,syntactic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved glove model\n",
    "loaded_neg_model=torch.load('skipgram_model.pth')\n",
    "loaded_neg_model.eval()  \n",
    "\n",
    "\n",
    "#extract the word vectors from the loaded model\n",
    "word_vectors_neg = loaded_neg_model.center_embedding.weight.data.numpy()\n",
    "word_vectors_neg\n",
    "#save the word vectors \n",
    "np.savetxt('skipgramNEG_word_vectors.txt', word_vectors_neg, delimiter=' ')\n",
    "\n",
    "#add word along with word vectors\n",
    "with open('skipgramNEG_word_vectors.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, word in enumerate(vocabs):\n",
    "        vector_neg = \" \".join(str(value) for value in word_vectors_neg[i])\n",
    "        f.write(f\"{word} {vector_neg}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_model = KeyedVectors.load_word2vec_format('skipgramNEG_word_vectors.txt', binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_analogy(neg_model,semantic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_analogy(neg_model,syntactic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved glove model\n",
    "loaded_glove_model = torch.load('glove_model.pth')\n",
    "loaded_glove_model.eval()\n",
    "\n",
    "#extract the word vectors from the loaded model\n",
    "word_vectors_glove = loaded_glove_model.center_embedding.weight.data.numpy()\n",
    "word_vectors_glove\n",
    "#save the word vectors \n",
    "np.savetxt('glove_word_vectors.txt', word_vectors_glove, delimiter=' ')\n",
    "\n",
    "#add word along with word vectors\n",
    "with open('glove_word_vectors.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, word in enumerate(vocabs):\n",
    "        vector = \" \".join(str(value) for value in word_vectors_glove[i])\n",
    "        f.write(f\"{word} {vector}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format('glove_word_vectors.txt', binary=False, no_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_analogy(glove_model,semantic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_analogy(glove_model,syntactic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gensim_model = KeyedVectors.load_word2vec_format('data/glove.6B.100d.txt', binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_analogy(gensim_model,semantic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5544871794871795"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_analogy(gensim_model,syntactic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model     | Window Size | Syntactic Accuracy| Semantic Accuracy|\n",
    "|:----------|:-----------:|:-----------------:|:----------------:|\n",
    "| Skipgram  |       2          |    0.0     |  0.0\n",
    "| Skipgram (NEG) |  2          |    0.0     |  0.0\n",
    "| Glove     |      2           |    0.0     |  0.0\n",
    "| Glove (Gensim)|   -          |    0.5545  | 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task2.3 Find the correlation between models dot product and human judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and add to test_data list\n",
    "test_data = []\n",
    "with open('data/wordsim353.txt') as file4:\n",
    "    for line in file4:\n",
    "        test_data.append(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list for keeping human judgment score\n",
    "human_judgment = []\n",
    "\n",
    "for l in test_data:\n",
    "    human_judgment.append(l[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's write a function to get embedding given a word\n",
    "def get_embed(model,word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    v_embed = model.center_embedding(id_tensor)\n",
    "    u_embed = model.outside_embedding(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (center_embedding): Embedding(14395, 2)\n",
       "  (outside_embedding): Embedding(14395, 2)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_model = torch.load('skipgram_model.pth')\n",
    "skipgram_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity of 2 words\n",
    "skipgram_similarity = []\n",
    "for line in test_data:\n",
    "    if line[1] in vocabs and line[2] in vocabs:\n",
    "        v = cos_sim(get_embed(skipgram_model,line[1]), get_embed(skipgram_model,line[2]))\n",
    "        skipgram_similarity.append(v)\n",
    "    else:\n",
    "        skipgram_similarity.append(0)\n",
    "\n",
    "# skipgram_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.052644236330194674"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print correlation value\n",
    "c1 = spearmanr(human_judgment, skipgram_similarity)\n",
    "c1.statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipgramNeg(\n",
       "  (center_embedding): Embedding(14395, 2)\n",
       "  (outside_embedding): Embedding(14395, 2)\n",
       "  (logsigmoid): LogSigmoid()\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgramNeg_model = torch.load('skipgramNEG_model.pth')\n",
    "skipgramNeg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity of 2 words\n",
    "skipgramNEG_similarity = []\n",
    "for line in test_data:\n",
    "    if line[1] in vocabs and line[2] in vocabs:\n",
    "        v = cos_sim(get_embed(skipgramNeg_model,line[1]), get_embed(skipgramNeg_model,line[2]))\n",
    "        skipgramNEG_similarity.append(v)\n",
    "    else:\n",
    "        skipgramNEG_similarity.append(0)\n",
    "\n",
    "# skipgramNEG_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0996954628699487"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print correlation value\n",
    "c2 = spearmanr(human_judgment, skipgramNEG_similarity)\n",
    "c2.statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Glove(\n",
       "  (center_embedding): Embedding(14395, 2)\n",
       "  (outside_embedding): Embedding(14395, 2)\n",
       "  (center_bias): Embedding(14395, 1)\n",
       "  (outside_bias): Embedding(14395, 1)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model = torch.load('glove_model.pth')\n",
    "glove_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity of 2 words\n",
    "glove_similarity = []\n",
    "for line in test_data:\n",
    "    if line[1] in vocabs and line[2] in vocabs:\n",
    "        v = cos_sim(get_embed(glove_model,line[1]), get_embed(glove_model,line[2]))\n",
    "        glove_similarity.append(v)\n",
    "    else:\n",
    "        glove_similarity.append(0)\n",
    "\n",
    "# glove_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0016354367549573797"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print correlation value\n",
    "c3 = spearmanr(human_judgment, glove_similarity)\n",
    "c3.statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_model = KeyedVectors.load_word2vec_format('data/glove.6B.100d.txt', binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list for keeping all vocabs on gensim\n",
    "gensim_vocabs = []\n",
    "with open('data/glove.6B.100d.txt',encoding=\"utf8\") as file4:\n",
    "    for line in file4:\n",
    "        gensim_vocabs.append(line.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_similarity = []\n",
    "for line in test_data:\n",
    "    if line[1] in gensim_vocabs and line[2] in gensim_vocabs:\n",
    "        v = 1-gensim_model.distance(line[1],line[2])\n",
    "        gensim_similarity.append(v)\n",
    "    else:\n",
    "        gensim_similarity.append(0)\n",
    "\n",
    "# gensim_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4276064647844679"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print correlation value\n",
    "c4 = spearmanr(human_judgment, gensim_similarity)\n",
    "c4.statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3 \n",
    "demo for web (create function and print 10 words that relate to input's word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Glove(\n",
       "  (center_embedding): Embedding(14395, 2)\n",
       "  (outside_embedding): Embedding(14395, 2)\n",
       "  (center_bias): Embedding(14395, 1)\n",
       "  (outside_bias): Embedding(14395, 1)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load saved glove model\n",
    "model = torch.load('glove_model.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to print 10 most similarity phr\n",
    "\n",
    "def similarity10(word_input):    \n",
    "    try:\n",
    "        # input word need to be 1 word\n",
    "        if len(word_input.split())==1:\n",
    "            word_emb=get_embed(model,word_input)\n",
    "            # add wikipedia data to list\n",
    "            data=[]\n",
    "            with open('app/harry-potter.txt') as file:\n",
    "                for word in file:\n",
    "                    data += word.split()\n",
    "            # calculate cosine similarity of input word and wikipedia data\n",
    "            # add to dict (key=word, value=cosine similarity)\n",
    "            similarity_dict = {}\n",
    "            for a in data:\n",
    "                if a in vocabs:\n",
    "                    a_emb = get_embed(model,a)\n",
    "                    value = cos_sim(word_emb,a_emb)\n",
    "                    similarity_dict[a] = value\n",
    "                else:\n",
    "                    continue\n",
    "            # sort the dict\n",
    "            similarity_dict_sorted = sorted(similarity_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "            # print the 10 most passages\n",
    "            for i in range(10):\n",
    "                print(f\"{i+1}.{similarity_dict_sorted[i][0]} ({similarity_dict_sorted[i][1]})\")\n",
    "        else:\n",
    "            print(\"the system can search with 1 word only\")\n",
    "                    \n",
    "    except:\n",
    "         print(\"the word is not in my corpus. Please enter the new word\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Harry (1.0)\n",
      "2.positive (0.9999996483942903)\n",
      "3.making (0.9984447307009964)\n",
      "4.other (0.9976278348341087)\n",
      "5.body (0.9970620531315026)\n",
      "6.among (0.9956205087068863)\n",
      "7.available (0.9888220151102337)\n",
      "8.of (0.9855963510450311)\n",
      "9.As (0.978467599111954)\n",
      "10.26 (0.9782186552403997)\n"
     ]
    }
   ],
   "source": [
    "similarity10('Harry')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
